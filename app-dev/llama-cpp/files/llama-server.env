# /etc/llama-server/llama-server.env
# model and bind
MODEL_PATH=/opt/llm-models/DeepSeek-R1-0528-Qwen3-8B-Q5_K_M.gguf

# sampling
TEMP=0.6
TOP_P=0.95

# runtime
THREADS=16
CTX_SIZE=8192
N_GPU_LAYERS=99

# batching and kv
EXTRA_FLAGS=--jinja --mlock --no-kv-offload --cache-type-k q8_0 --cache-type-v q8_0

# cuda and logging
GGML_CUDA_FORCE_MMQ=1
LLAMA_LOG_COLORS=1
